{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.48.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.5.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /opt/conda/lib/python3.7/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.10.28-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 33.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.25.9)\n",
      "Building wheels for collected packages: sentence-transformers, nltk, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101994 sha256=cbf4d82c696ffb0b6c13d098ffac4ba04af5131d4f04d6e33940880013d71096\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1c/43/65/fe0f3ea9327623e749a79eb5dfad85a809c84064b1cc4682c1\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=9d2c6c50c83b1b3a43823b9f61abc13474639ff88a80499886eefc234ce7fe40\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=f9ab3e8c115bf1ec60a8e1906f028f14a14874237fc7394742ad3971ca8882cb\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built sentence-transformers nltk sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, sentencepiece, transformers, nltk, sentence-transformers\n",
      "Successfully installed nltk-3.5 regex-2020.10.28 sacremoses-0.0.43 sentence-transformers-0.3.8 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 224\n",
    "\n",
    "def show_sample(sample):\n",
    "    \"\"\"\n",
    "    Displays a sample as they come out of the trainloader.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(sample['caption'], size=20)\n",
    "    ax1.imshow(sample['full_image'].permute(1,2,0))\n",
    "    ax2.imshow(sample['masked_image'].permute(1,2,0))\n",
    "    plt.show()\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotations, datadir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset of obfuscated coco images, with captions.\n",
    "        \n",
    "        annotations: load from pickle, akshay's processed annotations\n",
    "        datadir: Preprocessed data. Contains /originals and /masked\n",
    "        tranforms: function to be run on each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.annotations = annotations\n",
    "        self.filenames = os.listdir(datadir)\n",
    "        \n",
    "        # Since every 5 samples is the same image, we have a one image cache.\n",
    "        # TODO this may get fucky with shuffle? we can find out later.\n",
    "        self.last_image = None\n",
    "        self.last_index = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames) * 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gets images from the dataset.\n",
    "        \n",
    "        Each image has 5 replicas, with different captions and sections\n",
    "        \n",
    "        Returns: dictionary with blanked out ['image'] and ['caption']\n",
    "            image: FloatTensor\n",
    "            caption: string (may later be a list)\n",
    "        \"\"\"\n",
    "\n",
    "        # Load image or retrieve from cache\n",
    "        \n",
    "        image_filename = self.filenames[idx // 5]\n",
    "        image_id = int(image_filename.split(\".\")[0])\n",
    "        \n",
    "        \n",
    "        if self.last_index is not None and idx // 5 == self.last_index // 5:\n",
    "            full_image = self.last_image\n",
    "        else:\n",
    "            image_filepath = os.path.join(self.datadir, image_filename)\n",
    "            full_image = Image.open(image_filepath)\n",
    "            self.last_image = full_image\n",
    "        \n",
    "        self.last_index = idx\n",
    "        full_image = full_image.convert(\"RGB\") # The occasional 1 channel grayscale image is in there.\n",
    "        full_image = full_image.resize((image_dim, image_dim))\n",
    "\n",
    "        # Fetch annotation, mask out area\n",
    "        anno = self.annotations[image_id][idx % 5]\n",
    "        \n",
    "        masked_image = full_image.copy()\n",
    "        \n",
    "        draw = ImageDraw.Draw(masked_image)\n",
    "        draw.rectangle([(anno['coord_start'][0], anno['coord_start'][1]), (anno['coord_end'][0], anno['coord_end'][1])], fill=\"black\")\n",
    "\n",
    "        sample = {'masked_image': masked_image, 'caption': anno['caption'], 'full_image': full_image, 'image_id':image_id}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 191 ms, total: 1.57 s\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annos = pd.read_pickle(\"../annotations_train2017.pickle\")\n",
    "\n",
    "# Recommended resnet transforms.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "# TODO change masking logic to accomodate this\n",
    "#resnet_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), normalize, transforms.ToTensor()])\n",
    "#resnet_transform = transforms.Compose([transforms.Resize((image_dim,image_dim)), transforms.ToTensor(), normalize])\n",
    "resnet_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def basic_transform_sample(sample):\n",
    "    \"\"\"\n",
    "    A \"default\" transformer. Applies recommended resnet transforms.\n",
    "    \"\"\"\n",
    "    sample['masked_image'] = resnet_transform(sample['masked_image'])\n",
    "    sample['full_image'] = resnet_transform(sample['full_image'])\n",
    "    return sample\n",
    "\n",
    "dataset_train = COCODataset(annos, \"../data/train2017\", transform=basic_transform_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotfoNETv3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rotfoNETv3, self).__init__()\n",
    "        self.caption_encoder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "        for p in self.caption_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "            pass\n",
    "        \n",
    "        self.image_encoder = resnet50(pretrained=True)\n",
    "        for p in self.image_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "            pass\n",
    "        self.image_encoder.fc = nn.Linear(2048, 768)\n",
    "        self.merge_fc = nn.Linear(768*2, 256)\n",
    "        self.dropout = nn.Dropout(.35)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 8, 2, stride=2),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(8, 16, 5, stride=2),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, 5, stride=3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 10, stride=1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 7, stride=1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(8, 3, 7, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, caption):\n",
    "        encoded_caption = torch.Tensor(self.caption_encoder.encode(caption)).to(device)\n",
    "        encoded_img = self.image_encoder(img)\n",
    "        \n",
    "        x = torch.cat((encoded_caption, encoded_img), 1)\n",
    "        x = self.merge_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(-1, 1, 16, 16)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNRLoss(original, inpainted): \n",
    "    eps = 1e-11\n",
    "    mse = torch.mean((original - inpainted)**2)\n",
    "    psnr = 20 * torch.log10(1/torch.sqrt(mse+eps)) \n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 128\n",
    "lr = 1e-2\n",
    "\n",
    "trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=4) # VERY important to make sure num_workers > 0.\n",
    "\n",
    "model = rotfoNETv3().to(device)\n",
    "criterion = PSNRLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0533a59dc64846b189cd087e2d1f7a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c761a8de9fa47539d86134234cb4ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.035115420751273635\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epoch)):\n",
    "    sum_loss = 0\n",
    "    for i, batch in tqdm(enumerate(trainloader), total=len(dataset_train)//batch_size):\n",
    "        model.train()\n",
    "        masked_image = batch['masked_image'].to(device)\n",
    "        captions = batch['caption']\n",
    "        optimizer.zero_grad()\n",
    "        inpainted_image = model(masked_image, captions)\n",
    "        loss = criterion(inpainted_image, batch['full_image'].to(device))\n",
    "        sum_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        if i%500 ==0 and (not i == 0) :\n",
    "            print('loss: ', sum_loss/(i*batch_size))\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('avg SSIM loss: ', sum_loss/(len(dataset_train)//batch_size))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_index = np.random.randint(len(dataset_train))\n",
    "        sample_input = dataset_train[sample_index]\n",
    "\n",
    "        torch.save(model.state_dict(), 'ckpt_rotfoNETv3.pth')\n",
    "\n",
    "        save_image(sample_input['full_image'], './samples/v3_original_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "        save_image(sample_input['masked_image'], './samples/v3_original_masked_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "\n",
    "        inpainted_sample = model(sample_input['masked_image'].to(device).view(1, -1, 224, 224),[sample_input['caption']])\n",
    "        save_image(inpainted_sample, './samples/v3_inpainted_masked_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
