{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "cuda = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 224\n",
    "\n",
    "def show_sample(sample):\n",
    "    \"\"\"\n",
    "    Displays a sample as they come out of the trainloader.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = sample['mask']\n",
    "    \n",
    "    masked_im = sample['full_image'].clone()\n",
    "    masked_im[mask] = 0.0\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.suptitle(sample['caption'], size=20)\n",
    "    ax1.imshow(sample['full_image'].permute(1,2,0).numpy().clip(0,1))\n",
    "    ax2.imshow(masked_im.permute(1,2,0).numpy().clip(0,1))\n",
    "    ax3.imshow(sample['full_image'][mask].view(3,100,100).permute(1,2,0).numpy().clip(0,1))\n",
    "    plt.show()\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotations, datadir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset of obfuscated coco images, with captions.\n",
    "        \n",
    "        annotations: load from pickle, akshay's processed annotations\n",
    "        datadir: Preprocessed data. Contains /originals and /masked\n",
    "        tranforms: function to be run on each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.annotations = annotations\n",
    "        self.filenames = os.listdir(datadir)\n",
    "        \n",
    "        # Since every 5 samples is the same image, we have a one image cache.\n",
    "        # TODO this may get fucky with shuffle? we can find out later.\n",
    "        self.last_image = None\n",
    "        self.last_index = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames) * 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gets images from the dataset.\n",
    "        \n",
    "        Each image has 5 replicas, with different captions and sections\n",
    "        \n",
    "        Returns: dictionary with blanked out ['image'] and ['caption']\n",
    "            image: FloatTensor\n",
    "            caption: string (may later be a list)\n",
    "        \"\"\"\n",
    "\n",
    "        # Load image or retrieve from cache\n",
    "        \n",
    "        image_filename = self.filenames[idx // 5]\n",
    "        image_id = int(image_filename.split(\".\")[0])\n",
    "        \n",
    "        \n",
    "        if self.last_index is not None and idx // 5 == self.last_index // 5:\n",
    "            full_image = self.last_image\n",
    "        else:\n",
    "            image_filepath = os.path.join(self.datadir, image_filename)\n",
    "            full_image = Image.open(image_filepath)\n",
    "            self.last_image = full_image\n",
    "        \n",
    "        self.last_index = idx\n",
    "        full_image = full_image.convert(\"RGB\") # The occasional 1 channel grayscale image is in there.\n",
    "        full_image = full_image.resize((image_dim, image_dim))\n",
    "\n",
    "        # Fetch annotation, mask out area\n",
    "        anno = self.annotations[image_id][idx % 5]\n",
    "        \n",
    "        masked_image = full_image.copy()\n",
    "        \n",
    "        draw = ImageDraw.Draw(masked_image)\n",
    "        draw.rectangle([(anno['coord_start'][0], anno['coord_start'][1]), (anno['coord_end'][0], anno['coord_end'][1])], fill=\"black\")\n",
    "        \n",
    "        x1 = anno['coord_start'][0]\n",
    "        y1 = anno['coord_start'][1]\n",
    "        x2 = x1 + 100\n",
    "        y2 = y1 + 100\n",
    "        \n",
    "        mask = torch.zeros(3, image_dim,image_dim,dtype=torch.bool)\n",
    "        mask[:,y1:y2,x1:x2] = True\n",
    "\n",
    "        sample = {\n",
    "            'caption': anno['caption'],\n",
    "            'full_image': full_image, # Automatically stacked by the loader\n",
    "            'image_id':image_id,\n",
    "            'mask':mask # Automatically stacked by the loader\n",
    "         }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "annos = pd.read_pickle(\"../../annotations_train2017.pickle\")\n",
    "\n",
    "# Recommended resnet transforms.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "# TODO change masking logic to accomodate this\n",
    "#resnet_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), normalize, transforms.ToTensor()])\n",
    "#resnet_transform = transforms.Compose([transforms.Resize((image_dim,image_dim)), transforms.ToTensor(), normalize])\n",
    "resnet_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "def basic_transform_sample(sample):\n",
    "    \"\"\"\n",
    "    A \"default\" transformer. Applies recommended resnet transforms.\n",
    "    \"\"\"\n",
    "    #sample['masked_image'] = resnet_transform(sample['masked_image'])\n",
    "    sample['full_image'] = resnet_transform(sample['full_image'])\n",
    "    return sample\n",
    "\n",
    "dataset_train = COCODataset(annos, \"../../data/train2017\", transform=basic_transform_sample)\n",
    "trainloader = DataLoader(dataset_train, batch_size=64, shuffle=False, num_workers=2) # VERY important to make sure num_workers > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    #generator model\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        self.t1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.t2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.t3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.t4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.t5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "            \n",
    "        )\n",
    "#         self.t6=nn.Sequential(\n",
    "#             nn.Conv2d(512,4000,kernel_size=(4,4)), #bottleneck\n",
    "#             nn.BatchNorm2d(4000),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "        self.t7=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.t8=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.t9=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.t10=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.t11=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64,out_channels=3,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.t1(x)\n",
    "        x=self.t2(x)\n",
    "        x=self.t3(x)\n",
    "        x=self.t4(x)\n",
    "        x=self.t5(x)\n",
    "        #x=self.t6(x)\n",
    "        x=self.t7(x)\n",
    "        x=self.t8(x)\n",
    "        x=self.t9(x)\n",
    "        x=self.t10(x)\n",
    "        x=self.t11(x)\n",
    "        return x #output of generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    #discriminator model\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.t1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.t2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "\n",
    "        self.t3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.t4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.t5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=1,kernel_size=(4,4),stride=1,padding=0),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "        self.t6 = nn.Sequential(\n",
    "            nn.Linear(121, 121),\n",
    "            nn.Linear(121, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.t1(x)\n",
    "        x=self.t2(x)\n",
    "        x=self.t3(x)\n",
    "        x=self.t4(x)\n",
    "        x=self.t5(x)\n",
    "        x=self.t6(x)\n",
    "        #print(x.size())\n",
    "        return x #output of discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNRLoss(original, inpainted): \n",
    "    eps = 1e-11\n",
    "    mse = torch.mean((original - inpainted)**2)\n",
    "    psnr = 20 * torch.log10(1/torch.sqrt(mse+eps)) \n",
    "    return -psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50 # number of epochs of training\n",
    "batch_size = 64 # size of the batches\n",
    "n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "\n",
    "# Adam parameters\n",
    "lr = 0.0002 # adam: learning rate\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "\n",
    "#latent_dim = (3,1000,1000) # dimensionality of the latent space\n",
    "img_size = 64 # size of each image dimension\n",
    "channels = 3 # number of image channels\n",
    "sample_interval = 1000 # interval betwen image samples\n",
    "img_shape = (channels, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "adversarial_loss.cuda()\n",
    "\n",
    "adv_loss_weight = .5\n",
    "recon_loss_weight = .5\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "r_losses = []\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My weird way of getting a sample to eval on\n",
    "for b in trainloader:\n",
    "    m = b['masked_image']\n",
    "    f = b['full_image']\n",
    "    ids = b['image_id']\n",
    "    caps = b['caption']\n",
    "    break\n",
    "\n",
    "def show_checkpoint_repros():\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        res = generator(m.cuda())[0].cpu().permute(1,2,0)\n",
    "        masked = m[0].permute(1,2,0)\n",
    "        full = f[0].permute(1,2,0)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "    plt.suptitle(caps[0])\n",
    "    ax1.imshow(full)\n",
    "    ax2.imshow(masked)\n",
    "    ax3.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "for epoch in tqdm(range(1, n_epochs)):\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        \n",
    "        full_imgs = batch['full_image'].cuda()\n",
    "        mask = batch['mask']\n",
    "        masked_imgs = full_imgs.detach().clone()\n",
    "        \n",
    "        masked_imgs[mask] = 0.0\n",
    "        \n",
    "        # Adversarial ground truths - should be same size.\n",
    "        valid = Variable(Tensor(full_imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(masked_imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        #real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        #z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0],) +  latent_dim)))\n",
    "        #print(z.shape)\n",
    "\n",
    "        # Generate a batch of images\n",
    "\n",
    "        gen_imgs = generator(masked_imgs)\n",
    "        \n",
    "        gen_rois = gen_imgs[mask]\n",
    "        full_rois = full_imgs[mask]\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_adv_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        #recon_loss = PSNRLoss(full_imgs, gen_imgs)\n",
    "        recon_loss = mse(gen_rois, full_rois)\n",
    "        g_loss = recon_loss_weight*recon_loss + adv_loss_weight*g_adv_loss\n",
    "        \n",
    "#         print(recon_loss.item())\n",
    "#         print(g_adv_loss.item())\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "        #d_loss = \"N/A\"\n",
    "\n",
    "        if (epoch + i) % 2 == 0:\n",
    "            real_loss = adversarial_loss(discriminator(full_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(trainloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(str(datetime.now()), \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [Recon Loss: %f]\" % (epoch, n_epochs, i, len(trainloader), d_loss.item(), g_loss.item(), recon_loss.item()) )\n",
    "            #save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            r_losses.append(recon_loss.item())\n",
    "            show_checkpoint_repros()\n",
    "\n",
    "    torch.save(generator.state_dict(), f\"./checkpoints/GAN_psnr_generator_epoch_{epoch}_{str(g_loss.item())}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"./checkpoints/GAN_psnr_discriminator_epoch_{epoch}_{str(d_loss.item())}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.plot(g_losses, label=\"Generator Loss\")\n",
    "plt.xlabel(\"Checkpoint (500 batches)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
