{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 224\n",
    "\n",
    "def show_sample(sample):\n",
    "    \"\"\"\n",
    "    Displays a sample as they come out of the trainloader.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(sample['caption'], size=20)\n",
    "    ax1.imshow(sample['full_image'].permute(1,2,0))\n",
    "    ax2.imshow(sample['masked_image'].permute(1,2,0))\n",
    "    plt.show()\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotations, datadir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset of obfuscated coco images, with captions.\n",
    "        \n",
    "        annotations: load from pickle, akshay's processed annotations\n",
    "        datadir: Preprocessed data. Contains /originals and /masked\n",
    "        tranforms: function to be run on each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.annotations = annotations\n",
    "        self.filenames = os.listdir(datadir)\n",
    "        \n",
    "        # Since every 5 samples is the same image, we have a one image cache.\n",
    "        # TODO this may get fucky with shuffle? we can find out later.\n",
    "        self.last_image = None\n",
    "        self.last_index = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames) * 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gets images from the dataset.\n",
    "        \n",
    "        Each image has 5 replicas, with different captions and sections\n",
    "        \n",
    "        Returns: dictionary with blanked out ['image'] and ['caption']\n",
    "            image: FloatTensor\n",
    "            caption: string (may later be a list)\n",
    "        \"\"\"\n",
    "\n",
    "        # Load image or retrieve from cache\n",
    "        \n",
    "        image_filename = self.filenames[idx // 5]\n",
    "        image_id = int(image_filename.split(\".\")[0])\n",
    "        \n",
    "        \n",
    "        if self.last_index is not None and idx // 5 == self.last_index // 5:\n",
    "            full_image = self.last_image\n",
    "        else:\n",
    "            image_filepath = os.path.join(self.datadir, image_filename)\n",
    "            full_image = Image.open(image_filepath)\n",
    "            self.last_image = full_image\n",
    "        \n",
    "        self.last_index = idx\n",
    "        full_image = full_image.convert(\"RGB\") # The occasional 1 channel grayscale image is in there.\n",
    "        full_image = full_image.resize((image_dim, image_dim))\n",
    "\n",
    "        # Fetch annotation, mask out area\n",
    "        anno = self.annotations[image_id][idx % 5]\n",
    "        \n",
    "        masked_image = full_image.copy()\n",
    "        \n",
    "        draw = ImageDraw.Draw(masked_image)\n",
    "        draw.rectangle([(anno['coord_start'][0], anno['coord_start'][1]), (anno['coord_end'][0], anno['coord_end'][1])], fill=\"black\")\n",
    "\n",
    "        sample = {'masked_image': masked_image, 'caption': anno['caption'], 'full_image': full_image, 'image_id':image_id}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 188 ms, total: 2.03 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annos = pd.read_pickle(\"../annotations_train2017.pickle\")\n",
    "\n",
    "# Recommended resnet transforms.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "# TODO change masking logic to accomodate this\n",
    "#resnet_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), normalize, transforms.ToTensor()])\n",
    "#resnet_transform = transforms.Compose([transforms.Resize((image_dim,image_dim)), transforms.ToTensor(), normalize])\n",
    "resnet_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "def basic_transform_sample(sample):\n",
    "    \"\"\"\n",
    "    A \"default\" transformer. Applies recommended resnet transforms.\n",
    "    \"\"\"\n",
    "    sample['masked_image'] = resnet_transform(sample['masked_image'])\n",
    "    sample['full_image'] = resnet_transform(sample['full_image'])\n",
    "    return sample\n",
    "\n",
    "dataset_train = COCODataset(annos, \"../data/train2017\", transform=basic_transform_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotfoNETv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rotfoNETv1, self).__init__()\n",
    "        self.caption_encoder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "        for p in self.caption_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "            pass\n",
    "        \n",
    "        self.image_encoder = resnet50(pretrained=True)\n",
    "        for p in self.image_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "            pass\n",
    "        self.image_encoder.fc = nn.Linear(2048, 768)\n",
    "        self.merge_fc = nn.Linear(768*2, 256)\n",
    "        self.dropout = nn.Dropout(.35)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 8, 2, stride=2),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(8, 16, 5, stride=2),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(16, 32, 5, stride=3),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 10, stride=1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 7, stride=1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(8, 3, 7, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, caption):\n",
    "        encoded_caption = torch.Tensor(self.caption_encoder.encode(caption)).to(device)\n",
    "        encoded_img = self.image_encoder(img)\n",
    "        \n",
    "        x = torch.cat((encoded_caption, encoded_img), 1)\n",
    "        x = self.merge_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(-1, 1, 16, 16)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "\n",
    "trainloader = DataLoader(dataset_train, batch_size=32, shuffle=False, num_workers=4) # VERY important to make sure num_workers > 0.\n",
    "\n",
    "model = rotfoNETv1().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a757268168b941b3aaadf58846755ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4090ab2a7434190b96becc7d348c4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epoch)):\n",
    "    for i, batch in tqdm(enumerate(trainloader), total=round(len(dataset_train)/batch_size)):\n",
    "        model.train()\n",
    "        masked_image = batch['masked_image'].to(device)\n",
    "        captions = batch['caption']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inpainted_image = model( batch['masked_image'].to(device), captions)\n",
    "        loss = criterion(inpainted_image, batch['full_image'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_index = np.random.randrange(len(dataset_train))\n",
    "        sample_input = dataset_train[sample_index]\n",
    "        \n",
    "        torch.save(model.state_dict(), 'ckpt_rotfoNETv1.pth')\n",
    "        \n",
    "        save_image(sample_input['full_image'], './samples/original_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "        save_image(sample_input['masked_image'], './samples/original_masked_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "        \n",
    "        inpainted_sample = model(sample_input['masked_image'].to(device), sample_input['caption'])\n",
    "        save_image(inpainted_sample, './samples/inpainted_masked_{}_{}.png'.format(sample_input['image_id'], epoch))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
